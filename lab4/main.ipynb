{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Spam Classification\n",
    "\n",
    "The database contains SMS messages which are either spam or non-spam. The goal is to classify a message as spam or non-spam.\n",
    "\n",
    "The database contains 3734 training examples and 1840 testing examples. The non-spam:spam ratio is 6:1.\n",
    "\n",
    "Examples of sentences in the dataset:\n",
    "\n",
    "**spam** URGENT! We are trying to contact you. Last weekends draw shows that you have\n",
    "won a Â£900 prize GUARANTEED. Call 09061701939. Claim code S89. Valid 12hrs only\n",
    "\n",
    "**ham** Hi frnd, which is best way to avoid missunderstding wit our beloved one's?\n",
    "\n",
    "**ham** Great escape. I fancy the bridge but needs her lager. See you tomo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_sentences = np.load('data_lab5/data/training_sentences.npy', allow_pickle=True)\n",
    "train_labels = np.load('data_lab5/data/training_labels.npy')\n",
    "test_sentences = np.load('data_lab5/data/test_sentences.npy', allow_pickle=True)\n",
    "test_labels = np.load('data_lab5/data/test_labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISES\n",
    "\n",
    "### 2. Define a function `normalize_data(train_data, test_data, type=None)` having the parameters as follows: the training data, the testing data and the type of normalization (can be one of `{None, standard, l1, l2}`) and returns the normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_data(train_data, test_data, type=None):\n",
    "    if type is None:\n",
    "        return train_data, test_data\n",
    "\n",
    "    if type == 'l1' or type == 'l2':\n",
    "        scaler = Normalizer(norm=type)\n",
    "    elif type == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif type == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        return train_data, test_data\n",
    "\n",
    "    scaler.fit(train_data)\n",
    "    scaled_train_data = scaler.transform(train_data)\n",
    "    scaled_test_data = scaler.transform(test_data)\n",
    "\n",
    "    return scaled_train_data, scaled_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define the class BagOfWords whose constructor initializes the vocabulary (an empty dictionary).\n",
    "\n",
    "Inside the class, define the method `build_vocabulary(self, data)`, whose `data` parameter is a list of messages (a list of list of strings) and constructs the vocabulary based on the data.\n",
    "\n",
    "The dictionary's keys are the words and the values are unique IDs for each word.\n",
    "\n",
    "Further more, make a list of words inside the class, which contains the words in the reading order.\n",
    "\n",
    "Print the vocabulary length (must be 9522).\n",
    "\n",
    "**Note:** The vocabulary will be constructed based only on the training data.\n",
    "\n",
    "### 4. Define the method `get_features(self, data)` whose `data` parameter is a list of messages of dimension `num_samples` and returns a matrix of dimension `num_samples * dictionary_length` defined as:\n",
    "\n",
    "$features(sample_{idx}, word_{idx})$ = the number of apparitions of the word whose ID is $word_{idx}$ in the document (message) $sample_{idx}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BagOfWords:\n",
    "    def __init__(self):\n",
    "        self.vocabulary = dict()\n",
    "        self.words = list()\n",
    "\n",
    "    # ex3\n",
    "    def build_vocabulary(self, data):\n",
    "        self.vocabulary['UNK'] = 0\n",
    "        i = 1\n",
    "        for sentence in data:\n",
    "            for word in sentence:\n",
    "                if word not in self.vocabulary:\n",
    "                    self.vocabulary[word] = i\n",
    "                    self.words.append(word)\n",
    "                    i += 1\n",
    "\n",
    "        print(len(self.vocabulary))\n",
    "\n",
    "    # ex4\n",
    "    def get_features(self, data):\n",
    "        features = np.zeros((len(data), len(self.vocabulary)))\n",
    "        for idx, sentence in enumerate(data):\n",
    "            for word in sentence:\n",
    "                if word in self.vocabulary:\n",
    "                    features[idx, self.vocabulary[word]] += 1\n",
    "                else:\n",
    "                    features[idx, 0] += 1\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Using the functions defined above, calculate the BOW representation for the training data and testing data, then normalize them using L2 norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9523\n"
     ]
    }
   ],
   "source": [
    "bag = BagOfWords()\n",
    "bag.build_vocabulary(train_sentences)\n",
    "train_features = bag.get_features(train_sentences)\n",
    "test_features = bag.get_features(test_sentences)\n",
    "scaled_train_sentences, scaled_test_sentences = normalize_data(train_features, test_features, type='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.35355339 0.35355339 ... 0.         0.         0.        ]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "print(scaled_train_sentences[:1])\n",
    "print(scaled_test_sentences[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Train a SVM with linear kernel that classifies spam/non-spam messages. Use parameter C of value 1.\n",
    "\n",
    "Calculate the `accuracy` and `F1-score` for the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9847826086956522\n",
      "0.9423868312757202\n"
     ]
    }
   ],
   "source": [
    "svm_model = SVC()\n",
    "svm_model.C = 1\n",
    "svm_model.kernel = 'linear'\n",
    "\n",
    "# train - nonscaled data\n",
    "svm_model.fit(train_features, train_labels)\n",
    "\n",
    "# predict - nonscaled data\n",
    "predictions = svm_model.predict(test_features)\n",
    "print(accuracy_score(test_labels, predictions))\n",
    "print(f1_score(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9717391304347827\n",
      "0.8879310344827587\n"
     ]
    }
   ],
   "source": [
    "svm_model = SVC()\n",
    "svm_model.C = 1\n",
    "svm_model.kernel = 'linear'\n",
    "\n",
    "# train - scaled data\n",
    "svm_model.fit(scaled_train_sentences, train_labels)\n",
    "\n",
    "# predict - scaled data\n",
    "predictions = svm_model.predict(scaled_test_sentences)\n",
    "print(accuracy_score(test_labels, predictions))\n",
    "print(f1_score(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the 10 most negative (spam) words and 10 most positive (non-spam) words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Extra\n",
    "indexes = np.argsort(svm_model.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['&lt#&gt', 'me', 'i', 'Going', 'him', 'Ok', 'I', 'Ill', 'my', 'Im']\n"
     ]
    }
   ],
   "source": [
    "nonspam = [bag.words[x - 1] for x in indexes[:10]]\n",
    "print(nonspam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'To', 'mobile', 'CALL', 'FREE', 'txt', '&', 'Call', 'Txt', 'STOP']\n"
     ]
    }
   ],
   "source": [
    "spam = [bag.words[x - 1] for x in indexes[-10:]]\n",
    "print(spam)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
